{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5f848754",
      "metadata": {
        "id": "5f848754"
      },
      "source": [
        "# Optimization and Geometric Analysis of Functions in Two Variables\n",
        "\n",
        "In this activity, we will work with multivariable differential calculus using `sympy` and `matplotlib` to:\n",
        "- Calculate partial derivatives, the gradient, and the Hessian matrix of a two-variable function.\n",
        "- Identify and classify critical points by finding where the partial derivatives are zero.\n",
        "- Visualize the results graphically with a contour plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xQCJFte3okMA",
      "metadata": {
        "id": "xQCJFte3okMA"
      },
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd8e4a1",
      "metadata": {
        "id": "6dd8e4a1"
      },
      "source": [
        "## 1. Function Definition\n",
        "\n",
        "We define a symbolic function of two variables, $g(x, y)$, that has a clearly identifiable critical point. In this case:\n",
        "\n",
        "$$\n",
        "g(x, y) = x^2 + 3y^2 - 4x + 2y + 1\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65958e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the src directory to the Python path\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "\n",
        "from plotting_utils import plot_3d_surface_and_contour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8110561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "c8110561",
        "outputId": "8cbcd7cc-2c6a-463e-fff8-af604dfbdbc6"
      },
      "outputs": [],
      "source": [
        "# Define the symbolic variables\n",
        "x, y = sp.symbols('x y')\n",
        "\n",
        "# Define the multivariable function\n",
        "g = x**2 + 3*y**2 - 4*x + 2*y + 1\n",
        "\n",
        "g"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dhNN16xXsdHE",
      "metadata": {
        "id": "dhNN16xXsdHE"
      },
      "source": [
        "## 2. Partial Derivatives, Gradient, and Hessian Matrix\n",
        "\n",
        "### 1. Partial Derivatives\n",
        "\n",
        "The **partial derivatives** of a two-variable function $ f(x, y) $ are defined as the rate of change of the function with respect to one of its variables, while holding the other constant:\n",
        "\n",
        "- Partial derivative with respect to $ x $:\n",
        "  $$\n",
        "  \\frac{\\partial f}{\\partial x}(x, y) = \\lim_{h \\to 0} \\frac{f(x+h, y) - f(x, y)}{h}\n",
        "  $$\n",
        "\n",
        "- Partial derivative with respect to $ y $:\n",
        "  $$\n",
        "  \\frac{\\partial f}{\\partial y}(x, y) = \\lim_{h \\to 0} \\frac{f(x, y+h) - f(x, y)}{h}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Gradient $ \\nabla f $\n",
        "\n",
        "The **gradient** of a scalar function $ f(x, y) $ is a **vector** that contains all of its partial derivatives. It points in the direction of the **greatest rate of increase** of the function:\n",
        "\n",
        "$$\n",
        "\\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
        "$$\n",
        "\n",
        "- The gradient is **perpendicular** to the level curves of $ f $.\n",
        "- In optimization, $ \\nabla f = 0 $ indicates the **critical points**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Hessian Matrix $ H(f) $\n",
        "\n",
        "The **Hessian matrix** is a square matrix containing all the **second-order partial derivatives** of a function. It describes the **curvature** of $ f(x, y) $:\n",
        "\n",
        "$$\n",
        "H(f(x, y)) =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\\\\\n",
        "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MEVS9OsBtgth",
      "metadata": {
        "id": "MEVS9OsBtgth"
      },
      "source": [
        "First, we will determine each of these parameters for the function $g(x,y)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9607ac5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "f9607ac5",
        "outputId": "994473b2-8775-4e02-aeac-fac051b4f1e4"
      },
      "outputs": [],
      "source": [
        "# Partial derivatives\n",
        "dg_dx = sp.diff(g, x)\n",
        "dg_dy = sp.diff(g, y)\n",
        "\n",
        "# Gradient\n",
        "grad_g = sp.Matrix([dg_dx, dg_dy])\n",
        "\n",
        "# Hessian Matrix\n",
        "hess_g = sp.hessian(g, (x, y))\n",
        "\n",
        "print(\"Partial derivative with respect to x:\\n∂g/∂x:\", dg_dx)\n",
        "print(\"\\nPartial derivative with respect to y:\\n∂g/∂y:\", dg_dy)\n",
        "print(\"\\nGradient of g(x,y):\")\n",
        "display(grad_g)\n",
        "print(\"\\nHessian Matrix of g(x,y):\")\n",
        "display(hess_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WK7zq49OmoMu",
      "metadata": {
        "id": "WK7zq49OmoMu"
      },
      "source": [
        "Using the `.solve()` method from `sympy`, we can determine the critical points of the function $g(x,y)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9479ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9479ba",
        "outputId": "66abf26a-0b0b-41c9-c272-9eb401673125"
      },
      "outputs": [],
      "source": [
        "critical_points = sp.solve([dg_dx, dg_dy], (x, y))\n",
        "critical_points # Returns a dict with the critical points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7621d62",
      "metadata": {
        "id": "d7621d62"
      },
      "source": [
        "## 3. Critical Point Classification\n",
        "\n",
        "We evaluate the Hessian matrix at the critical point and analyze its eigenvalues to classify the point:\n",
        "- **All eigenvalues are positive**: local minimum\n",
        "- **All eigenvalues are negative**: local maximum\n",
        "- **Mixed (positive and negative) eigenvalues**: saddle point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9e5ae4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "db9e5ae4",
        "outputId": "088dda2f-c36e-4b49-c4ce-b8f9f3df9cf8"
      },
      "outputs": [],
      "source": [
        "# Substitute the critical points into the Hessian matrix\n",
        "H_at_crit = hess_g.subs(critical_points)\n",
        "\n",
        "# Returns a dict {keys: eigenvalues, vals: algebraic multiplicity}\n",
        "eigenvals = H_at_crit.eigenvals()\n",
        "\n",
        "print(\"Hessian Matrix at the critical point:\")\n",
        "display(H_at_crit)\n",
        "print(\"\\nEigenvalues of the Hessian Matrix:\", eigenvals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e839fdc5",
      "metadata": {
        "id": "e839fdc5"
      },
      "source": [
        "In this case, the eigenvalues are positive, so the critical point is a **local minimum**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1bdf9d",
      "metadata": {
        "id": "9d1bdf9d"
      },
      "source": [
        "## 4. Visualization\n",
        "\n",
        "To visualize the function, we will plot the surface of $g(x, y)$ and a contour map, marking the critical point on both graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1924684e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "1924684e",
        "outputId": "6e384d12-533a-4ce8-c6c1-f952bf5cb006"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "plot_3d_surface_and_contour(g, critical_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0568c153",
      "metadata": {
        "id": "0568c153"
      },
      "source": [
        "## Why is multivariable calculus important in Machine Learning?\n",
        "\n",
        "- It is especially used in training algorithms like gradient descent: calculating the gradient helps determine the direction to adjust the model's parameters to minimize a loss function.\n",
        "- The Hessian matrix is fundamental in more advanced optimization methods (like Newton-Raphson), as it provides information about the curvature of the function.\n",
        "- Classifying a function's critical points helps identify optimal or unstable minima and maxima (saddle points), which is crucial for the model's convergence to useful solutions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
